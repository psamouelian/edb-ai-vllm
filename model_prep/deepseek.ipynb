{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724ae535-b029-414a-924b-8ad09f7df73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import botocore\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98987dd-e5b1-4302-9689-095970834c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64d8e7b-0776-475c-9415-c36d61db2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup variables for this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df4bf28-4764-4c64-8ed3-c1491e5e1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_models_path = './models'\n",
    "model_prefix = \"deepseek-ai\"\n",
    "model_name = \"DeepSeek-R1-Distill-Llama-8B\"\n",
    "full_model_name = f\"{model_prefix}/{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a182279-f5af-4dd2-94da-cf51d0e32df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26c50eae-2c6a-43fd-9b8a-752774307a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5816c4cf-ca43-4534-9c94-f034ec38ab39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80368c552da541ef8d19d6d4f4a9ed05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(full_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(full_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "model.generation_config = GenerationConfig.from_pretrained(full_model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73819ddb-758c-447a-bbff-062c573446cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a7b2e2-ae0b-47dc-ae63-a65f23388714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform a quick inference test on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a0d7355-12b9-49fe-8b39-8da57d21c987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I'm trying to figure out how old the universe is. I remember hearing something about the Big Bang and the universe expanding, but I'm not exactly sure about the details. Let me think through this step by step.\n",
      "\n",
      "First, I know that the universe started with a Big Bang, which is when it began to expand from an extremely small and incredibly dense point. But how long ago was that? I think it's called the age of the universe, and scientists have a way to\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How old is the universe?\"}\n",
    "]\n",
    "input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "#outputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\n",
    "outputs = model.generate(input_tensor.to(model.device), max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a817c4b-a5d0-422f-ba3c-cad3f8365fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b90a80-5a46-4dc2-8bff-b4cb058cbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model and its tokenizer to a local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd93b4e0-96c2-4a92-869f-7d1db6a04061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t\t  model-00004-of-00004.safetensors\n",
      "generation_config.json\t\t  model.safetensors.index.json\n",
      "model-00001-of-00004.safetensors  special_tokens_map.json\n",
      "model-00002-of-00004.safetensors  tokenizer_config.json\n",
      "model-00003-of-00004.safetensors  tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "save_path = Path.home().joinpath(root_models_path, model_name)\n",
    "!mkdir -p {save_path}\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "!ls {save_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7ac09-94f0-4e8c-9f5d-52015bcc4535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491cad34-4900-4505-8776-80cdbaf10f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the s3 connection parameters (these are entered in the workbench setup screens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37ef171-1870-42ee-823e-658a3a54ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "region_name = os.environ.get('AWS_DEFAULT_REGION')\n",
    "bucket_name = os.environ.get('AWS_S3_BUCKET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b70f71-b948-4ead-8c80-5bfd6676c7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d05b31cc-a3e8-499d-9161-9266f4b498e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session(\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1cc7ae9-07b0-403e-a9d9-7444c4707081",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = session.resource(\n",
    "               's3',\n",
    "               config=botocore.client.Config(signature_version='s3v4'),\n",
    "               endpoint_url=endpoint_url,\n",
    "               region_name=region_name\n",
    "              )\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "                        \n",
    "def upload_directory_to_s3(local_directory, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(file_path, local_directory)\n",
    "            s3_key = os.path.join(s3_prefix, relative_path)\n",
    "            print(f\"{file_path} -> {s3_key}\")\n",
    "            bucket.upload_file(file_path, s3_key)\n",
    "                                \n",
    "def list_objects(prefix):\n",
    "    filter = bucket.objects.filter(Prefix=prefix)\n",
    "    for obj in filter.all():\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ce2dd-f335-4ed2-b9d7-d879acb747f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ad5eae7-d994-43d6-a6de-698952a2a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload the model to an S3 bucket within the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b4f7204-a725-45a3-8308-bba6b0787413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/DeepSeek-R1-Distill-Llama-8B/model.safetensors.index.json -> deepseek-ai/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B/model.safetensors.index.json\n",
      "./models/DeepSeek-R1-Distill-Llama-8B/tokenizer_config.json -> deepseek-ai/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B/tokenizer_config.json\n",
      "./models/DeepSeek-R1-Distill-Llama-8B/model-00004-of-00004.safetensors -> deepseek-ai/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B/model-00004-of-00004.safetensors\n",
      "./models/DeepSeek-R1-Distill-Llama-8B/model-00002-of-00004.safetensors -> deepseek-ai/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B/model-00002-of-00004.safetensors\n",
      "./models/DeepSeek-R1-Distill-Llama-8B/special_tokens_map.json -> deepseek-ai/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B/special_tokens_map.json\n",
      "./models/DeepSeek-R1-Distill-Llama-8B/model-00003-of-00004.safetensors -> deepseek-ai/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B/model-00003-of-00004.safetensors\n",
      "./models/DeepSeek-R1-Distill-Llama-8B/tokenizer.json -> deepseek-ai/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B/tokenizer.json\n",
      "./models/DeepSeek-R1-Distill-Llama-8B/generation_config.json -> deepseek-ai/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B/generation_config.json\n",
      "./models/DeepSeek-R1-Distill-Llama-8B/config.json -> deepseek-ai/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B/config.json\n",
      "./models/DeepSeek-R1-Distill-Llama-8B/model-00001-of-00004.safetensors -> deepseek-ai/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B/model-00001-of-00004.safetensors\n"
     ]
    }
   ],
   "source": [
    "upload_directory_to_s3(root_models_path, full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bcfc8c-3fb6-4201-b201-fd90c36c19eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
